[run]
name = accurate_vdvae_novdvaeskip_softplus_noresdist_mnist_23layer_32z_32bs
seed = 420

# Hardware
# Global run config for GPUs and CPUs
num_gpus = 1
num_cpus = 256

max_allowed_checkpoints = 5

parallel_shards = 128


[data]
# Source. Can be one of ('binarized_mnist', 'cifar-10', 'imagenet', 'celebA', 'celebAHQ', 'ffhq')
dataset_source = 'binarized_mnist'

# only used for data source (ffhq, celebAHQ, celebA)
train_data_path = '../celebAHQ/train_data/'
val_data_path = '../celebAHQ/val_data/'
synthesis_data_path = '../celebAHQ/val_data/'

# Image dimensions
target_res = 32
channels = 1
num_bits = 8.
random_horizontal_flip = True


[model]
# General
# Main experimentation params
stable_init = True
initialize_prior_weights_as_zero = False
use_1x1_conv = True

# min std value of the gaussian layer
# base can be in ('std', 'logstd'). Determines if the model should predict std or logstd.
distribution_base = 'std'
# Gradient smoothing beta. ln(2) ~= 0.6931472
gradient_smoothing_beta = 0.6931472

################################# Layers' structure parameters ####################################
# In the bottom-up block, a skip connection is only taken once every n_blocks_per_res residual blocks
# That skip connection is linked to all top-down blocks of the matching resolution defined by backwards index in down_strides
# Example: up_strides = [x, a, x, x] and down_strides = [y, y, b, y] | the skip from a is linked to all blocks of b.

# Both downsampling and upsampling is done in tandem with residual blocks in a downsample-last upsample-first manner.
# i.e, both the downsample block and the upsample block are considered to belong to the higher resolution.
# Example: up_strides = [2, 2, 4] and up_n_blocks_per_res = [5, 9, 3] means there are in reality [6, 10, 4] blocks in the respective resolutions
# Similarly for down_strides = [4, 2, 2] and down_n_blocks_per_res = [3, 9, 5] means there are in reality [6, 10, 4] blocks.

# Bottom-up block
# Input conv
input_conv_filters = 32
input_kernel_size = (1, 1)

# Level up blocks
up_strides = [2] + [1, 2] + [1] * 9 + [2] + [1] * 4 + [4] + [1] * 5
up_n_blocks_per_res = [0] * 23
up_n_blocks = [1] * 23
up_n_layers = [2] * 23
up_filters = [32] + [64, 64] + [128] * 9 + [128] + [256] * 4 + [256] + [512] * 5
up_mid_filters_ratio = [1.] * 23
up_kernel_size = [3] * 18 + [1] * 5
up_skip_filters = [384] * 23

# Top-down block
use_residual_distribution = False

# Level down blocks
down_strides = [1] * 5 + [4] + [1] * 4 + [2] + [1] * 9 + [2, 1] + [2]
down_n_blocks_per_res = [0] * 23
down_n_blocks = [1] * 23
down_n_layers = [2] * 23
down_filters = [512] * 5 + [256] + [256] * 4 + [128] + [128] * 9 + [64, 64] + [32]
down_mid_filters_ratio = [1.] * 23
down_kernel_size = [1] * 5 + [3] * 18
down_latent_variates = [32] * 23

# Output conv
output_kernel_size = (1, 1)
num_output_mixtures = 10
##################################################################################################


[loss]
min_mol_logscale = -14.

# lambda of variational prior loss
# schedule can be in ('None', 'Logistic', 'Linear')
variation_schedule = 'Linear'

# linear beta schedule
vae_beta_anneal_start = 21
vae_beta_anneal_steps = 500
vae_beta_min = 1e-4

# Gamma schedule of variational loss
use_gamma_schedule = True
gamma_max_steps = 1000
scaled_gamma = True

# logistic beta schedule
vae_beta_activation_steps = 10000
vae_beta_growth_rate = 1e-5

#L2 weight decay
use_weight_decay = True
l2_weight = 1e-2


[metrics]
latent_active_threshold = 1e-4


[optimizer]
# Generator
# Optimizer can be one of ('Radam', 'Adam', 'Adamax', 'SGD')
type = 'Adamax'
learning_rate_scheme = 'cosine'
learning_rate = 1e-3

# noam/constant/cosine warmup
warmup_steps = 100.

# exponential or cosine
decay_steps = 200000
decay_start = 5000
min_learning_rate = 1e-4

# exponential only
decay_rate = 0.5

# Adam
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Gradient
# Gradient clip_norm value is for pixel independent gradient (unit), it will be automatically multiplied by resolution during the run
clip_gradient_norm = False
gradient_clip_norm_value = 20.

gradient_skip = True
gradient_skip_threshold = 800.


[init]
# Solely for model initialization
batch_size = 1

[train]
total_train_steps = 3000000
batch_size = 32

ema_decay = 0.9999
resume_from_ema = False

logging_interval_in_steps = 1

checkpoint_and_eval_interval_in_steps = 10000


[val]
n_samples_for_validation = 5000
batch_size = 32 * 4


[synthesis]
temperature_settings = [0.8, 0.85, 0.9, 1., ('linear', 0.7, 0.9), ('linear', 0.9, 0.7), ('linear', 0.8, 1.), ('linear', 1., 0.8), ('linear', 0.8, 0.9)]
output_temperature = 1.

# can be one of ('reconstruction', 'generation', 'div_stats', 'encoding')
synthesis_mode = 'reconstruction'
load_ema_weights = True

# div_stats mode
div_stats_subset_ratio = 0.2

# reconstruction/encoding mode
save_target_in_reconstruction = False
mask_reconstruction = False
variate_masks_quantile = 0.03

# generation_mode
n_generation_batches = 1

batch_size = 256