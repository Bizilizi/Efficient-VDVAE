[run]
name = ffhq1024_baseline

seed = 69
#accurate_vdvae_vdvaeskip_softplus_noresdist_celeba_62layer_32z
# Hardware
# Global run config for GPUs and CPUs
num_gpus = 8
num_cpus = 256


[data]
# Source. Can be one of ('ffhq', 'celebaHQ', 'celeba', 'cifar-10', 'binarized_mnist', 'imagenet')
dataset_source = 'ffhq'

train_data_path = '../datasets/ffhq/train_data/'
val_data_path = '../datasets/ffhq/val_data/'
synthesis_data_path = '../datasets/ffhq/val_data/'

# Image dimensions
target_res = 1024
channels = 3
num_bits = 8.
# Flip is not used in MNIST
random_horizontal_flip = True


[model]
# General
# Main experimentation params
stable_init = True

# Generally not changed
initialize_prior_weights_as_zero = False

use_1x1_conv = True
# min std value of the gaussian layer
# base can be in ('std', 'logstd'). Determines if the model should predict std or logstd.
distribution_base = 'std'
gradient_smoothing_beta = 0.4

################################# Layers' structure parameters ####################################
# In the bottom-up block, a skip connection is only taken once every n_blocks_per_res residual blocks
# That skip connection is linked to all top-down blocks of the matching resolution defined by backwards index in down_strides
# Example: up_strides = [x, a, x, x] and down_strides = [y, y, b, y] | the skip from a is linked to all blocks of b.

# Both downsampling and upsampling is done in tandem with residual blocks in a downsample-last upsample-first manner.
# i.e, both the downsample block and the upsample block are considered to belong to the higher resolution.
# Example: up_strides = [2, 2, 4] and up_n_blocks_per_res = [5, 9, 3] means there are in reality [6, 10, 4] blocks in the respective resolutions
# Similarly for down_strides = [4, 2, 2] and down_n_blocks_per_res = [3, 9, 5] means there are in reality [6, 10, 4] blocks.

#Bottom-up block
# Bottom-up block
# Input conv
input_conv_filters = 8
input_kernel_size = (1, 1)

# Level up blocks
# Level up blocks
up_strides = [2] + [1, 2] + [1] * 2 + [2] + [1] * 4 + [2] + [1] * 6 + [2] + [1] * 11 + [2] + [1] * 7 + [2] + [1] * 4 + [2] + [1] * 3 + [4] + [1] * 2
up_n_blocks_per_res = [0] * 49
up_n_blocks = [1] * 49
up_n_layers = [2] * 49
up_filters = [16, 16] + [32] * 3 + [64] * 5 + [128] * 7 + [256] * 12 + [512] * 20
up_mid_filters_ratio = [1.] + [1., 1.] + [0.5] * 3 + [0.25] * 43
up_kernel_size = [3] * 47 + [1] * 2
up_skip_filters = [8] + [16, 16] + [32] * 3 + [64] * 5 + [128] * 7 + [256] * 12 + [512] * 19

# Top-down block
use_residual_distribution = False

# Level down blocks
# Level down blocks
down_strides = [1] * 2 + [4] + [1] * 3 + [2] + [1] * 4 + [2] + [1] * 7 + [2] + [1] * 11 + [2] + [1] * 6 + [2] + [1] * 4 + [2] + [1] * 2 + [2, 1] + [2]
down_n_blocks_per_res = [0] * 2 + [0] + [0] * 3 + [0] + [0] * 4 + [0] + [0] * 7 + [0] + [0] * 11 + [6] + [0] * 6 + [8] + [0] * 4 + [3] + [0] * 2 + [3, 0] + [3]
down_n_blocks = [1] * 49
down_n_layers = [2] * 49
down_filters = [512] * 19 + [256] * 12 + [128] * 7 + [64] * 5 + [32] * 3 + [16, 16] + [8]
down_mid_filters_ratio = [0.25] * 43 + [0.5] * 3 + [1., 1.] + [1.]
down_kernel_size = [1] * 2 + [3] * 47
down_latent_variates = [64] * 49

# Output conv
output_kernel_size = (1, 1)
num_output_mixtures = 10

[loss]

# feature matching
min_mol_logscale = -250.

# lambda of variational prior loss
# schedule can be in ('None', 'Logistic', 'Linear')
variation_schedule = 'Linear'

# linear beta schedule
vae_beta_anneal_start = 21
vae_beta_anneal_steps = 5000
vae_beta_min = 1e-4

# Gamma schedule of variational loss
use_gamma_schedule = True
gamma_max_steps = 10000
scaled_gamma = True

# logistic beta schedule
vae_beta_activation_steps = 10000
vae_beta_growth_rate = 1e-5

#L2 weight decay
use_weight_decay = False
l2_weight = 1e-6

[metrics]
latent_active_threshold = 1e-4

[optimizer]
# Generator
# Optimizer can be one of ('Radam', 'Adam', 'Adamax', 'SGD')
# Pre-pend 'Sam-' to the optimizer name to use SAM optimizer on top
type = 'Adamax'
learning_rate_scheme = 'cosine'
learning_rate = 1e-3

# noam/constant/cosine warmup
warmup_steps = 100.

# exponential or cosine
decay_steps = 850000
decay_start = 50000
min_learning_rate = 1e-4

# exponential only
decay_rate = 0.5
staircase = False

# Adam
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8

# Gradient
# Gradient clip_norm value is for pixel independent gradient (unit), it will be automatically multiplied by resolution during the run
clip_gradient_norm = False
gradient_clip_norm_value = 300.

gradient_skip = True
gradient_skip_threshold = 800.


[init]
# Solely for model initialization
batch_size = 1

[train]
total_train_steps = 3000000
batch_size = 1

ema_decay = 0.9999
resume_from_ema = False

checkpoint_and_eval_interval_in_steps = 10000

[val]
n_samples_for_validation = 1000
batch_size = 1


[synthesis]
temperature_settings = [0.8, 0.85, 0.9, 1., ('linear', 0.85, 1.), [0.85] *6 + [0.9]* 32 + [1.] * 21 ]
output_temperature = 1.

# can be one of ('reconstruction', 'generation', 'debug', 'encode', 'div_stats)
synthesis_mode = 'reconstruction'
load_ema_weights = True

# div_stats_mode
div_stats_subset_ratio = 0.20

save_target_in_reconstruction = False

# reconstruction / encode mode
mask_reconstruction = True
variates_masks_quantile = 0.03
# generation mode
n_generation_batches = 1
batch_size = 32
